import asyncio
from urllib.parse import urlparse
from wsgiref import headers
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime


# fiscoã®è¨˜äº‹ã‚’æ¢ã—å‡ºã™ãŸã‚ã®RSSURL
RSS_URL_MARKET_NEWS = "https://jp.investing.com/rss/news_25.rss"

# è¨˜äº‹å–å¾—ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ©ã‚°ï¼ˆæœ¬æ—¥ã™ã§ã«å–å¾—æ¸ˆã¿ã‹ï¼‰
last_fetched_date = None

# æ ¼ç´ç”¨
checked_urls = set()    

# ãƒ•ã‚£ã‚¹ã‚³ã®æ—¥çµŒãƒ¬ãƒ³ã‚¸ã‚’å–å¾—ã™ã‚‹
async def fetch_fisco_nikkei_range_exp():
    d = feedparser.parse(RSS_URL_MARKET_NEWS)
    debug_count_fisco_articles = 0
    for entry in d.entries:
        url = entry["link"]
        # fiscoã®è¨˜äº‹ã‹ã¤æœªãƒã‚§ãƒƒã‚¯ã®URLã®å ´åˆã€ä¸­èº«ã‚’è¦‹ã«è¡Œã
        if "fisco" in entry["author"].lower() and url not in checked_urls:
            debug_count_fisco_articles+=1  # ãƒ‡ãƒãƒƒã‚°ç”¨
            # è¨˜äº‹ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            response = requests.get(url, headers = {
                "Host": "jp.investing.com",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64; rv:56.0) Gecko/20100101 Firefox/56.0",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Accept-Encoding": "gzip, deflate, br",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Te": "trailers",
                "Connection": "keep-alive",
            })
            if response.status_code == 200:
                # ç¢ºèªæ¸ˆã¿URLã«è¿½åŠ 
                checked_urls.add(entry["link"])

                # HTML ã‚’ãƒ‘ãƒ¼ã‚¹
                soup = BeautifulSoup(response.text, "html.parser")

                # <div id="article"> å†…ã®å†…å®¹ã‚’å–å¾—
                article_div = soup.find("div", id="article")
                if article_div:
                    article_text = article_div.get_text(separator="\n", strip=True)

                    # è¨˜äº‹å†…ã«ã€Œ[æœ¬æ—¥ã®æƒ³å®šãƒ¬ãƒ³ã‚¸]ã€ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if "[æœ¬æ—¥ã®æƒ³å®šãƒ¬ãƒ³ã‚¸]" in article_text:
                        print("âœ… è¨˜äº‹ã«[æœ¬æ—¥ã®æƒ³å®šãƒ¬ãƒ³ã‚¸]ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼")
                        print("è¨˜äº‹ã®å†…å®¹:")
                        print(article_text)

                        # è¨˜äº‹å–å¾—æˆåŠŸ â†’ ãã®æ—¥ã¯ä»¥é™ã®å–å¾—ã‚’è¡Œã‚ãªã„ã‚ˆã†è¨˜éŒ²
                        last_fetched_date = datetime.now().date()
                        break  # 1ä»¶å–å¾—ã§ããŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    else:
                        print("âš  è¨˜äº‹ã«[æœ¬æ—¥ã®æƒ³å®šãƒ¬ãƒ³ã‚¸]ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                else:
                    print("âš  è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                print(f"âš  ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}ï¼‰")
                print("ğŸ”¹ [HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆæƒ…å ± ]")
                print(f"  - ãƒ¡ã‚½ãƒƒãƒ‰: {response.request.method}")
                print(f"  - URL: {response.request.url}")
                print(f"  - ãƒ˜ãƒƒãƒ€ãƒ¼:")
                for key, value in response.request.headers.items():
                    print(f"    {key}: {value}")

                if response.request.body:
                    print(f"  - ãƒœãƒ‡ã‚£:")
                    print(response.request.body.decode('utf-8', 'ignore'))  # ãƒœãƒ‡ã‚£ãŒã‚ã‚‹å ´åˆã«å‡ºåŠ›

                print("ğŸ”¹ [HTTP ãƒ¬ã‚¹ãƒãƒ³ã‚¹æƒ…å ±]")
                print(f"  - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
                print(f"  - ãƒ˜ãƒƒãƒ€ãƒ¼:")
                for key, value in response.headers.items():
                    print(f"    {key}: {value}")

                print(f"  - ãƒœãƒ‡ã‚£:\n{response.text[:500]}...")
            print("-" * 80)  # åŒºåˆ‡ã‚Šç·š
    print(f"fiscoã®æ—¥çµŒäºˆæƒ³ãƒ¬ãƒ³ã‚¸ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\næ¤œç´¢è¨˜äº‹æ•°:{debug_count_fisco_articles}")


asyncio.run(fetch_fisco_nikkei_range_exp())